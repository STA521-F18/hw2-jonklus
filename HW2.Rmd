---
title: "HW2 STA521 Fall18"
author: "Jonathan Klus jkk31"
date: "September 23, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exploratory Data Analysis

```{r data, echo = FALSE, include = FALSE}
library(alr3)
library(car)
library(dplyr)
library(knitr)
library(GGally)
library(ggplot2)
library(ggpubr)

data(UN3, package="alr3")
help(UN3) 
```


1. Create a summary of the data.  How many variables have missing data?  Which are quantitative and which are qualtitative?

```{r}
# quick summary - examine data types/structure
str(UN3)
```

```{r}
# check for missing data
missing_data = is.na(UN3)
summary(missing_data)
```

Out of the seven variables in the UN3 data set, six are missing at least one
data point. The only variable that is not missing data is Purban. 

All of the variables in this data set are quantitative. 

```{r}
summary(UN3)
```

2. What is the mean and standard deviation of each quantitative predictor?  Provide in a nicely formatted table.

```{r}
UN3_num = select(UN3,c(ModernC,Change,PPgdp,Frate,Pop,Fertility,Purban))
UN3_mean = apply(UN3_num,2,mean,na.rm=TRUE)
UN3_sd = apply(UN3_num,2,sd,na.rm=TRUE)
UN3_summary = data.frame(mean = UN3_mean, sd= UN3_sd)
kable(UN3_summary,"markdown",c(2,2),row.names = TRUE)
```


3. Investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots
highlighting the relationships among the predictors. Comment
on your findings regarding trying to predict `ModernC` from the other variables.  Are there potential outliers, nonlinear relationships or transformations that appear to be needed based on your graphical EDA?

```{r, echo = FALSE, warning = FALSE}
# initial EDA
ggpairs(UN3)
```

```{r}
y_frate = ggplot(data = UN3, aes(x = Frate, y = ModernC)) + geom_point(na.rm = TRUE) + 
  labs(title = "Contraceptive Use vs. Adult Female Economic Activity")

y_purban = ggplot(data = UN3, aes(x = Purban, y = ModernC)) + geom_point(na.rm = TRUE) + 
  labs(title = "Contraceptive Use vs. % Urban Pop")

y_pop = ggplot(data = UN3, aes(x = Pop, y = ModernC)) + geom_point(na.rm = TRUE) + 
  labs(title = "Contraceptive Use vs. Population")

ggarrange(y_frate, y_purban, y_pop, labels = c("A","B", "C"), nrow = 2, ncol = 2)
#also try gridExtra::grid.arrange


```

There are some noteable potential issues in using this data to predict ModernC.
In Chart A, there are potential outliers where countries have very high rates of
females over age 15 who are economically active (Frate) but zero or near-zero
percent use of modern contraception by unmarried women. In Chart B, the relationship between 
percent urban population and use of modern contraception appears fairly linear.
In Chart C, the Pop variable includes several extreme values, and it appears
that the data would benefit from a log transformation to scale the values
more appropriately. The two most extreme values, China and India, would likely
be considered outliers.

## Model Fitting

4.  Use the `lm()` function to perform a multiple linear regression with `ModernC` as the response and all other variables as the predictors, using the formula `ModernC ~ .`, where the `.` includes all remaining variables in the dataframe.  Create  diagnostic residual plot from the linear model object and comment on results regarding assumptions.  How many observations are used in your model fitting?

```{r}
UN3_clean = na.omit(UN3)
contra_lm = lm(ModernC ~ Change + PPgdp + Frate + Pop + Fertility + Purban, UN3)
summary(contra_lm)
#par(mfrow=c(2,2))
plot(contra_lm)
```

There were 125 observations used in this model fitting. That is calculated from
df = 118, with 7 degrees of freedom lost to the estimation of the intercept and
six coefficients (7 parameters). So n = 118 + 7 = 125. 

The diagnostic plots indicate:
- Residuals vs. Fitted Values: There are a few potential outliers, notably Poland,
Azerbijan, and the Cook Islands. The model generally appears to meet the 
homoskedasticity assumption, though the presence of fewer fitted values with 
smaller residuals towards the lower end of the y axis may make justify 
some concern about heteroskedacity. 
- Normal Q-Q: The residuals generally adhere to the normal assumption, with 
some deviation in the upper tail. 
- Scale-Location: The standardized residuals are not unduly large (i.e. no 
observations >2), but there does appear to be some pattern to the residuals
(i.e. smaller fitted values having smaller residuals than larger fitted values).
This pattern is not extreme, but may be cause for concern as we consider 
transformations for the final model (i.e. should we log transform the response
to try and control what appears to be a mild case of heteroskedasticity?)

5. Examine added variable plots `car::avPlot` or `car::avPlots`  for your model above. Are there any plots that suggest that transformations are needed for any of the terms in the model? Describe. Is it likely that any of the localities are influential for any of the terms?  Which localities?  Which terms?  

```{r}
avPlots(contra_lm)
```

The plot for population with all other variables held constant has points for 
India and China that are very likely influential points and outliers. They are 
from the mean of the data and appear to influence the slope of the regression line.
Both Pop and PPgdp may benefit from a log transformation, as their data is quite
spread out.

6.  Using the Box-Tidwell  `car::boxTidwell` or graphical methods find appropriate transformations of the predictor variables to be used as predictors in the linear model.  If any predictors are negative, you may need to transform so that they are non-negative.  Describe your method and  the resulting transformations.

Here, the Box-Tidwell method is performed on the three variables that appear
that they may benefit most from a possible transformation, based upon their
graphical appearance in the added variable plots above. 


```{r}
boxTidwell(ModernC ~ PPgdp + Pop + Fertility, ~Change + Purban + Frate, UN3_clean)
```

```{r}
boxTidwell(ModernC ~ Frate + Fertility , ~log(PPgdp) + log(Pop) + Change + Purban, UN3_clean)
```

7. Given the selected transformations of the predictors, select a transformation of the response using `MASS::boxcox` or `car::boxCox` and justify.

The Box-Cox tests both produce a MLE for lambda near zero, which suggests that
a log transformation may be appropriate. The scatterplot for ModernC vs. Pop
suggests that the scaling may be improved by a log transformation. The Box Cox
suggests that the response variable should remain level. 

```{r}
ggplot(UN3_clean,aes(x=log(Pop), y=ModernC))+geom_point() + geom_smooth(method="lm")
ggplot(UN3_clean,aes(x=log(PPgdp), y=ModernC))+geom_point() + geom_smooth(method="lm")
```


```{r}
contra_lm_Xtransf = lm(ModernC ~ log(PPgdp) + log(Pop) + Fertility + Change + Frate + Purban, UN3_clean)
summary(contra_lm_Xtransf)
plot(contra_lm_Xtransf)

boxCox(contra_lm_Xtransf)
```

8.  Fit the regression using the transformed variables.  Provide residual plots and added variables plots and comment.  If you feel that you need additional transformations of either the response or predictors, repeat any steps until you feel satisfied.
```{r}
avPlots(contra_lm_Xtransf)
```

The transformed model is specified and plotted above. The transformations have especially
helped to scale some of the observations that appeared to be potentially influential. 
Based on the slope of the added variable plot for Purban, it does not appear to be 
contributing much additional information to the model. 

9. Start by finding the best transformation of the response and then find transformations of the predictors.  Do you end up with a different model than in 8?


```{r}
boxCox(contra_lm)
```

By changing the order in which we perform Box Cox and Box Tidwell, we do end up
with the same suggested model as in the previous example. This is because the 
Box Cox procedure has a maximum likelihood of approximatey 3/4, and an interval 
that includes 1. Therefore we elect not to transform the response. The result
of Box Tidwell is then the same as in #6 above. 

10.  Are there any outliers or influential points in the data?  Explain.  If so, refit the model after removing any outliers and comment on residual plots.

```{r}
outlierTest(contra_lm_Xtransf)
```

There do not appear to be any outliers remaining in the model. The test for
outliers (with Bonferroni correction for multiple testing) does not return 
any points with p < 0.05. 

## Summary of Results

11. For your final model, provide summaries of coefficients with 95% confidence intervals in a nice table with interpretations of each coefficient.  These should be in terms of the original units! 


```{r}
CI = confint(contra_lm_Xtransf)
CI_w_coeff = data.frame(contra_lm_Xtransf$coefficients,CI[,1],CI[,2])
kable(CI_w_coeff,'markdown',c(2,2,2),row.names = TRUE, col.names = c("Coeff","Lower","Upper"))
```
- Intercept: When all predictors are 0, a country is predicted to have 4.12 ModernC. 
- log(PPgdp): A 1% increase in PPgdp is associated with a .055 unit increase in ModernC. 
- log(Pop):
- Fertility:
- Change:
- Frate:
- Purban: 

12. Provide a paragraph summarizing your final model  and findings suitable for the US envoy to the UN after adjusting for outliers or influential points.   You should provide a justification for any case deletions in your final model

The final version of this model uses 

## Methodology

    
13. Prove that the intercept in the added variable scatter plot will always be zero.  _Hint:  use the fact that if $H$ is the project matrix which contains a column of ones, then $1_n^T (I - H) = 0$.  Use this to show that the sample mean of residuals will always be zero if there is an intercept.




14. For multiple regression with more than 2 predictors, say a full model given by `Y ~ X1 + X2 + ... Xp`   we create the added variable plot for variable `j` by regressing `Y` on all of the `X`'s except `Xj` to form `e_Y` and then regressing `Xj` on all of the other X's to form `e_X`.  Confirm that the slope in a manually constructed added variable plot for one of the predictors  in Ex. 10 is the same as the estimate from your model. 
